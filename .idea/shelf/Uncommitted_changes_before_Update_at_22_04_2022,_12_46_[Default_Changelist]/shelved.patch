Index: AN_analyses.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import argparse\nimport spacy\nimport os\nimport numpy as np\nimport pandas as pd\nimport utils\nfrom collections import Counter\nfrom wordfreq import word_frequency\n\n# Pandas settings\npd.set_option('display.max_columns', None)\n\n# Run with arguments, for example: --exercise 3\nparser = argparse.ArgumentParser()\nparser.add_argument('--data_dir', default='data/preprocessed/train/', help=\"Directory containing the dataset\")\nparser.add_argument('--data_dir_stat', default='data/original/english/', help=\"Directory containing the Wiki dataset\")\nparser.add_argument('--exercise', default='all')\n\ndef get_words_data(text):\n    words, lens = [], []\n    for token in text:\n        if token.is_punct != True:\n            words.append(token.text)\n            lens.append(len(token.text))\n    if len(lens) != 0:\n        avg_len = sum(lens) / len(lens)\n    else:\n        avg_len = 0\n    return len(words), avg_len\n\ndef get_average_num_words(text):\n    sentences = list(text.sents)\n    avg = []\n    for sent in sentences:\n        avg.append(get_words_data(sent)[0])\n    return sum(avg) / len(avg)\n\ndef tokenization():\n    # The number of tokens such as words, numbers, punctuation marks etc.\n    tokens = [token.text for token in doc]\n    print(\"Number of tokens: %i\" % len(tokens))\n    # The number of unique tokes\n    print(\"Number of types: %i\" % len(set(tokens)))\n    words_data = get_words_data(doc)\n    # The number of words after removing stopwords and punctuations\n    print(\"Number of words: %i\" % words_data[0])\n    # The number of average words per sentence\n    print(\"Average number of words per sentence: %.2f\" % get_average_num_words(doc))\n    print(\"Average word length: %.2f\" % words_data[1])\n    print(\"\\n\")\n\ndef words():\n    tags = [token.tag_ for token in doc]\n\n    # Get 10 most frequent tags\n    unique_elements, frequency = np.unique(tags, return_counts=True)\n    sorted_indexes = np.argsort(frequency)[::-1]\n    fgPOS = unique_elements[sorted_indexes][:10]\n    freq = frequency[sorted_indexes][:10]\n\n    freq_tokens, infreq_tokens, uPOS = [], [], []\n    for tag in fgPOS:\n        # Get most frequent and infrequent words with that tag\n        words = [token.text for token in doc if token.tag_ == tag]\n        words_tally = Counter(words)\n        freq_tokens.append(', '.join([word for word, cnt in words_tally.most_common(3)]))\n        infreq_tokens.append(words_tally.most_common()[-1][0])\n        # Get POS for that tag\n        uPOS.append(next(token.pos_ for token in doc if token.tag_ == tag))\n\n    # Build DataFrame for output\n    word_class = pd.DataFrame({'Fg POS-tag': fgPOS,\n                               'Universal POS-tag': uPOS,\n                               'Occurrences': freq[:10],\n                               'Relative Tag Freq(%)': np.around(freq[:10] / len(tags),2),\n                               '3 most frequent tokens': freq_tokens,\n                               'Example infrequent token': infreq_tokens})\n    print(word_class)\n    print(\"\\n\")\n\ndef get_ngram(text, ngram):\n    temp = zip(*[text[i:] for i in range(0, ngram)])\n    return [' '.join(ngram) for ngram in temp]\n\ndef ngrams():\n    tokens = [token.text for token in doc]\n    bigram_tokens = Counter(get_ngram(tokens, 2))\n    trigram_tokens = Counter(get_ngram(tokens, 3))\n\n    pos = [token.tag_ for token in doc]\n    bigram_pos = Counter(get_ngram(pos, 2))\n    trigram_pos = Counter(get_ngram(pos, 3))\n\n    #TO-DO: Maike, Giulia - speak about Maike taking unigrams for bigrams and bigrams for trigrams\n    print('Token bigrams: ', bigram_tokens.most_common(3))\n    print('Token trigrams: ', trigram_tokens.most_common(3))\n    print('POS bigrams: ', bigram_pos.most_common(3))\n    print('POS trigrams:', trigram_pos.most_common(3))\n    print('\\n')\n\ndef lemmatization():\n    tokens = {}\n    sentences = {}\n    for sentence in doc.sents:\n        for token in sentence:\n            if (token.lemma_ != token.text.lower()):  # then there is an inflection\n                if token.lemma_ not in tokens.keys():\n                    tokens[token.lemma_] = [token.text]\n                    sentences[token.lemma_] = [sentence]\n                else:\n                    # if infliction did not exist, add to list\n                    if token.text not in tokens[token.lemma_]:\n                        tokens[token.lemma_].append(token.text)\n                        sentences[token.lemma_].append(sentence)\n                if len(tokens[token.lemma_]) == 3:\n                    print('Lemma: ', token.lemma_)\n                    print('Inflected Forms: ', tokens[token.lemma_])\n                    print('Example sentences for each form: ', sentences[token.lemma_])\n                    print('\\n')\n                    return\n\ndef ner():\n    ne, ne_labels = [], []\n    for ent in doc.ents:\n        ne.append(ent.text)\n        ne_labels.append(ent.label_)\n    print('Number of named entities: ', len(ne))\n    #TO-DO Why is Maike counting the unique NE?\n    print('Number of unique named entities: ', len(set(ne)))\n    print('Number of different entity labels: ', len(set(ne_labels)))\n\n    for i, sentence in enumerate(doc.sents):\n        print(sentence)\n        print('Named entities: ', [ent.text for ent in sentence.ents])\n        if i==4: break\n\ndef process_wiki():\n    global wiki_data\n    wiki_data.columns = ['target', 'cna', 'cnna', 'bin', 'prob']\n    wiki_data['cannotators'] = wiki_data.cna + wiki_data.cnna\n    wiki_data['tokens'] = wiki_data.target.apply(lambda x: nlp(x))\n    wiki_data['ntokens'] = wiki_data.tokens.apply(lambda x: len(x))\n\ndef explore_dataset():\n    text = 'Both China and the Philippines flexed their muscles on Wednesday.'\n    target = 'flexed their muscles'\n    target_pos = text.find(target)\n    print('Start and offset for target \"' + target + '\": ' + str(target_pos) + ' ' + str(target_pos + len(target)))\n\n    target = 'flexed'\n    target_pos = text.find(target)\n    print('Start and offset for target \"' + target + '\": ' + str(target_pos) + ' ' + str(target_pos + len(target)))\n\ndef basic_stat():\n    #7746 in total\n    print('Number of instances labeled with 0: %i' % len(wiki_data[wiki_data.bin == 0]))\n    print('Number of instances labeled with 1: %i' % len(wiki_data[wiki_data.bin == 1]))\n    print('Min, max, median, mean, and stdev of the probabilistic label: %.2f, %.2f, %.2f, %.2f, %.2f' % (\n        wiki_data.prob.min(), wiki_data.prob.max(), wiki_data.prob.median(), wiki_data.prob.mean(), wiki_data.prob.std()\n    ))\n    print('Number of instances consisting of more than one token: %i' % len(wiki_data[wiki_data.ntokens != 1]))\n    print('Maximum number of tokens for an instance: %i' % max(wiki_data.ntokens))\n\ndef ling_char():\n    global wiki_data\n    # Filter to take only #tokens = 1 and at least one complex annotation\n    wiki_data = wiki_data[(wiki_data.ntokens == 1) & (wiki_data.cannotators > 1)]\n    wiki_data['len_tokens'] = wiki_data.tokens.apply(lambda x: len(x[0]))\n    wiki_data['freq_tokens'] = wiki_data.tokens.apply(lambda x: word_frequency(str(x[0]), 'en'))\n    wiki_data['pos_tag'] = wiki_data.tokens.apply(lambda x: x[-1].pos_)\n    print(len(wiki_data))\n\n    print('Pearson correlation length and complexity: ', round(wiki_data.len_tokens.corr(wiki_data.prob),2))\n    print('Pearson correlation frequency and complexity: ', round(wiki_data.freq_tokens.corr(wiki_data.prob), 2))\n\n    utils.save_plot(wiki_data.len_tokens, wiki_data.prob, 'length of tokens', 'probabilistic complexity',\n                 'Probabilistic complexity by length of tokens', 'len_tokens_prob_scatter.png', 'images/')\n    utils.save_plot(wiki_data.freq_tokens, wiki_data.prob, 'frequency of tokens', 'probabilistic complexity',\n                 'Probabilistic complexity by frequency of tokens', 'freq_tokens_prob_scatter.png', 'images/')\n    utils.save_plot(wiki_data.pos_tag, wiki_data.prob, 'POS tag', 'probabilistic complexity',\n                 'Probabilistic complexity by POS tags', 'pos_tags_prob_scatter.png', 'images/')\n\nif __name__ == '__main__':\n    \"\"\"\n        `Data Analysis`\n    \"\"\"\n    # Load the parameters\n    args = parser.parse_args()\n    args_dict = vars(args)\n\n    if args.exercise == 'all':\n        exercise = range(1, 9)\n    else:\n        exercise = [int(args.exercise)]\n\n    # Load the data\n    data_file = open(args.data_dir + \"sentences.txt\", encoding=\"utf8\", errors='ignore')\n    data = data_file.read().replace('\\n', '')\n    data_file.close()\n\n    # Load English tokenizer, tagger, parser and NER\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(data)\n\n    # Load and Process Wiki Data\n    if any(ex in range(7,9) for ex in exercise):\n        nlp = spacy.load(\"en_core_web_sm\")\n        wiki_data = pd.read_csv(args.data_dir_stat + \"WikiNews_Train.tsv\", sep='\\t', header=None, usecols=[4, 7, 8, 9, 10])\n        process_wiki()\n\n    # Load function names for each exercise\n    functions = {1: tokenization, 2: words, 3: ngrams, 4: lemmatization, 5: ner,\n                 6: explore_dataset, 7: basic_stat, 8: ling_char}\n\n    for ex in exercise:\n        functions[ex]()\n\n\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/AN_analyses.py b/AN_analyses.py
--- a/AN_analyses.py	(revision 33a7bb4ce7072041bcfbb418dac7b3bb022f9ace)
+++ b/AN_analyses.py	(date 1650614101704)
@@ -3,7 +3,7 @@
 import os
 import numpy as np
 import pandas as pd
-import utils
+import matplotlib.pyplot as plt
 from collections import Counter
 from wordfreq import word_frequency
 
@@ -14,7 +14,10 @@
 parser = argparse.ArgumentParser()
 parser.add_argument('--data_dir', default='data/preprocessed/train/', help="Directory containing the dataset")
 parser.add_argument('--data_dir_stat', default='data/original/english/', help="Directory containing the Wiki dataset")
-parser.add_argument('--exercise', default='all')
+parser.add_argument('--exercise', default='5')
+
+#Figure settings
+fig_folder = "images/"
 
 def get_words_data(text):
     words, lens = [], []
@@ -161,6 +164,17 @@
     print('Number of instances consisting of more than one token: %i' % len(wiki_data[wiki_data.ntokens != 1]))
     print('Maximum number of tokens for an instance: %i' % max(wiki_data.ntokens))
 
+def save_scatter(x, y, xlabel, ylabel, title, plot_name):
+    if not os.path.exists(fig_folder):
+        os.makedirs(fig_folder)
+    fig = plt.figure(figsize=(8, 5))
+    plt.scatter(x, y)
+    plt.title(title)
+    plt.xlabel(xlabel)
+    plt.ylabel(ylabel)
+    plt.show()
+    fig.savefig(fig_folder + plot_name, dpi=fig.dpi)
+
 def ling_char():
     global wiki_data
     # Filter to take only #tokens = 1 and at least one complex annotation
@@ -173,12 +187,12 @@
     print('Pearson correlation length and complexity: ', round(wiki_data.len_tokens.corr(wiki_data.prob),2))
     print('Pearson correlation frequency and complexity: ', round(wiki_data.freq_tokens.corr(wiki_data.prob), 2))
 
-    utils.save_plot(wiki_data.len_tokens, wiki_data.prob, 'length of tokens', 'probabilistic complexity',
-                 'Probabilistic complexity by length of tokens', 'len_tokens_prob_scatter.png', 'images/')
-    utils.save_plot(wiki_data.freq_tokens, wiki_data.prob, 'frequency of tokens', 'probabilistic complexity',
-                 'Probabilistic complexity by frequency of tokens', 'freq_tokens_prob_scatter.png', 'images/')
-    utils.save_plot(wiki_data.pos_tag, wiki_data.prob, 'POS tag', 'probabilistic complexity',
-                 'Probabilistic complexity by POS tags', 'pos_tags_prob_scatter.png', 'images/')
+    save_scatter(wiki_data.len_tokens, wiki_data.prob, 'length of tokens', 'probabilistic complexity',
+                 'Probabilistic complexity by length of tokens', 'len_tokens_prob_scatter.png')
+    save_scatter(wiki_data.freq_tokens, wiki_data.prob, 'frequency of tokens', 'probabilistic complexity',
+                 'Probabilistic complexity by frequency of tokens', 'freq_tokens_prob_scatter.png')
+    save_scatter(wiki_data.pos_tag, wiki_data.prob, 'POS tag', 'probabilistic complexity',
+                 'Probabilistic complexity by POS tags', 'pos_tags_prob_scatter.png')
 
 if __name__ == '__main__':
     """
Index: TODO_analyses.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from decimal import DivisionByZero\nfrom importlib.resources import open_text\nfrom shutil import which\nimport spacy \nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nfrom nltk import ngrams\nfrom nltk import FreqDist\n\ntraining_data_file = open(\"data/preprocessed/train/sentences.txt\", \"r\")\ntraining_data  = training_data_file.read()\ntraining_data_file.close()\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(training_data)\n\nwhich_exercise = [1,2,3] # you can change this to only run one or less for efficiency :)\n\n# 1 -----------------------\nif 1 in which_exercise:\n    # computes word frequences, outputs dict.\n    def numFrequencies(doc):\n        word_frequencies = Counter()\n        for sentence in doc.sents:\n            words = []\n            for token in sentence: \n                # Let's filter out punctuation\n                if not token.is_punct:\n                    words.append(token.text)\n            word_frequencies.update(words)\n        return word_frequencies\n\n    # Number of tokens \n    def numTokens(doc):\n        return len(doc)\n\n    def numWordsAndTypes(doc):\n        word_frequencies = numFrequencies(doc)\n        return sum(word_frequencies.values()) , len(word_frequencies.keys())\n\n\n    # Number of types : all the words without punctuation and repeat\n    # Number of words : all the words without punctuation etc \n    num_tokens = numTokens(doc)\n    num_words, num_types = numWordsAndTypes(doc)\n\n\n    # Average number of words per sentence\n    sentences = doc.sents\n    num_words_sentence_all = []\n    for sentence in sentences:\n        num_words_sentence, num_types_sentence = numWordsAndTypes(sentence)\n        num_words_sentence_all.append(num_words_sentence)\n\n    mean_num_words_per_sentence = np.mean(num_words_sentence_all)\n\n    # Average word length\n    all_words = numFrequencies(doc).keys()\n    word_length = []\n    for word in all_words:\n        word_length.append(len(word))\n\n    mean_word_lenth = np.mean(word_length)\n\n\n    print(f\"Number of tokens: \\n {num_tokens} \\n Number of types: \\n {num_types} \\n Number of words: \\n {num_words} \\n Average number of words per sentence: \\n {mean_num_words_per_sentence} \\n Average word length: {mean_word_lenth}\\n\")\n\n\nif 2 in which_exercise:\n    ## 2 -----------------------\n    # DO THEY ONLY WANT WORDS OR ALSO PUNCTIATION\n    fPOS_frequencies = Counter()\n    fPOS = []\n    # dictionary for fine --> unif.\n    unfPOS_dict = {}\n    for sentence in doc.sents:\n        for token in sentence:\n            fPOS.append(token.tag_)\n            if token.tag_ not in unfPOS_dict.keys():\n                unfPOS_dict[token.tag_] = token.pos_\n\n    fPOS_frequencies.update(fPOS)\n    #print(fPOS_frequencies)\n\n    fPOS10 = []\n    occ10 = []\n    tag_freq = []\n    for fPOStag, cnt in fPOS_frequencies.most_common(10):\n        fPOS10.append(fPOStag) #Finegrained POS tag\n        occ10.append(cnt) # occurancy of tag\n        tag_freq.append((cnt/num_tokens)) # frequency of tag\n\n    def findWordsTag(doc, tag): \n        word_frequencies_tag = Counter()\n        for sentence in doc.sents:\n            words_tag= []\n            for token in sentence: \n                if token.tag_ == tag:\n                    words_tag.append(token.text)\n            word_frequencies_tag.update(words_tag)\n        return word_frequencies_tag\n\n    # find 3 most and least frequent words with tags in fPOS10\n    most_freq_tokens = []\n    least_freq_tokens = []\n    unfPOS10 = []\n    for tag in fPOS10:\n        unfPOS10.append(unfPOS_dict[tag])\n        freq_token_tag = findWordsTag(doc, tag)\n        freq_tokens = []\n        print(f\"for tag {tag} the most common are : \\n {freq_token_tag.most_common(3)} \")\n        for token, cnt in freq_token_tag.most_common(3):\n            freq_tokens.append(token)\n        most_freq_tokens.append((tuple(freq_tokens)))\n        least_freq_tokens.append(freq_token_tag.most_common()[:-2:-1][0][0])\n\n    most_freq_words_tag =  findWordsTag(doc, fPOS10[0])\n    class_table = pd.DataFrame({'Finegrained POS-tag' : fPOS10 ,'Universal POS-tag' : unfPOS10, 'Occurances': occ10, 'Relative Tag Frequency (%)' : tag_freq, '3 most frequent tokens' : most_freq_tokens, 'Infrequent token': least_freq_tokens} )\n    print(class_table)\n#3 ------------------------\nif 3 in which_exercise:\n    # generate bigrams trigrams\n    bigram_token=[]\n    trigram_token=[]\n    bigram_POS=[]\n    trigram_POS=[]\n    bigram_POS_frequencies = Counter()\n    trigram_POS_frequencies = Counter()\n    bigram_token_frequencies = Counter()\n    trigram_token_frequencies = Counter()\n    for sentence in doc.sents:\n        for idx in range(len(sentence) - 2):\n            bigram_token.append((sentence[idx].text, sentence[idx + 1].text))\n            trigram_token.append((sentence[idx].text, sentence[idx + 1].text, sentence[idx + 2].text ))\n            bigram_POS.append((sentence[idx].tag_, sentence[idx + 1].tag_))\n            trigram_POS.append((sentence[idx].tag_, sentence[idx + 1].tag_, sentence[idx + 2].tag_ ))\n        \n        bigram_token.append((sentence[len(sentence) - 2].text,sentence[len(sentence) - 1].text))\n        bigram_token.append((sentence[len(sentence) - 1].text))\n        trigram_token.append((sentence[len(sentence) - 2].text, sentence[len(sentence) - 1].text))\n        bigram_POS.append((sentence[len(sentence) - 2].tag_,sentence[len(sentence) - 1].tag_))\n        bigram_POS.append((sentence[len(sentence) - 1].tag_))\n        trigram_POS.append((sentence[len(sentence) - 2].tag_, sentence[len(sentence) - 1].tag_))\n    bigram_POS_frequencies.update(bigram_POS)\n    trigram_POS_frequencies.update(trigram_POS)\n    bigram_token_frequencies.update(bigram_token)\n    trigram_token_frequencies.update(trigram_token)\n\n    most_common_bigram_POS = bigram_POS_frequencies.most_common(3)\n    most_common_trigram_POS =  trigram_POS_frequencies.most_common(3)\n    most_common_bigram_token = bigram_token_frequencies.most_common(3)\n    most_common_trigram_token = trigram_token_frequencies.most_common(3)\n    print(f\"Token bigrams:\\n {most_common_bigram_POS } \\n Token trigrams: \\n {most_common_trigram_POS } \\n POS bigrams: \\n {most_common_bigram_token} \\n POS trigrams: \\n {most_common_trigram_token} \\n\")\n\n\n# 4.\tLemmatization (1 point)\n# Provide an example for a lemma that occurs in more than two inflections in the dataset. \n# Lemma:\n# Inflected Forms: \n# Example sentences for each form: \n\n\n\n#-----------\nif 4 in which_exercise:\n    def find3inflictions(doc):\n        tokens = {}\n        sentences = {}\n        for sentence in doc.sents:\n            for token in sentence:\n                if (token.lemma_ != token.text): #then there is an infiction\n                    if token.lemma_ not in tokens.keys():\n                        tokens[token.lemma_] = [token.text]\n                        sentences[token.lemma_] = [sentence]\n                    else:\n                        # if infliction did not exist, add to list\n                        if token.text not in tokens[token.lemma_]:\n                            tokens[token.lemma_].append(token.text)\n                            sentences[token.lemma_].append(sentence)\n                    if len(tokens[token.lemma_]) == 3:\n                        return token.lemma_, tokens[token.lemma_], sentences[token.lemma_]\n        return None\n\n    lemma, words, sentences = find3inflictions(doc)  \n    print(lemma)\n    print(words)\n    print(f\"Lemma: {lemma},\\n Inflected Forms: {words}, \\n Example sentences for each form: \\n {sentences}\")\n\n#--- \nif 5 in which_exercise:\n    freq_ents = Counter()\n    freq_labels = Counter()\n    for sentence in doc.sents:\n        for ent in sentence.ents:\n            freq_ents.update([ent.text])\n            freq_labels.update([ent.label_])\n        num_ents = len(freq_ents)\n        num_labels = len(freq_labels)\n    print(f\"Number of named entities: {num_ents}, \\n Number of different entity labels:  {num_labels}\")\n\n# it still counts enter (\\\\) as entity and i dunno whyy\n    # print(freq_ents)\n    # print(freq_labels)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TODO_analyses.py b/TODO_analyses.py
--- a/TODO_analyses.py	(revision 33a7bb4ce7072041bcfbb418dac7b3bb022f9ace)
+++ b/TODO_analyses.py	(date 1650612796843)
@@ -14,7 +14,7 @@
 nlp = spacy.load('en_core_web_sm')
 doc = nlp(training_data)
 
-which_exercise = [1,2,3] # you can change this to only run one or less for efficiency :)
+which_exercise = [5] # you can change this to only run one or less for efficiency :)
 
 # 1 -----------------------
 if 1 in which_exercise:
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"4e5420f3-a0e5-4b71-8c71-0dab952bcd35\" name=\"Default Changelist\" comment=\"Minor changes\" />\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"FileTemplateManagerImpl\">\n    <option name=\"RECENT_TEMPLATES\">\n      <list>\n        <option value=\"Python Script\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n  </component>\n  <component name=\"JupyterTrust\" id=\"2cfff2f0-98f0-4495-a566-164be147bd42\" />\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProjectId\" id=\"25jqgH0NDSIIQdptWy8GrxgTj4d\" />\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\">\n    <property name=\"RunOnceActivity.OpenProjectViewOnStart\" value=\"true\" />\n    <property name=\"RunOnceActivity.ShowReadmeOnStart\" value=\"true\" />\n    <property name=\"SHARE_PROJECT_CONFIGURATION_FILES\" value=\"true\" />\n    <property name=\"WebServerToolWindowFactoryState\" value=\"false\" />\n    <property name=\"last_opened_file_path\" value=\"$PROJECT_DIR$\" />\n    <property name=\"settings.editor.selected.configurable\" value=\"com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable\" />\n  </component>\n  <component name=\"RecentsManager\">\n    <key name=\"CopyFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$\" />\n    </key>\n    <key name=\"MoveFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$/data/preprocessed/train\" />\n      <recent name=\"$PROJECT_DIR$/data/preprocessed/test\" />\n      <recent name=\"$PROJECT_DIR$/data/preprocessed/val\" />\n    </key>\n  </component>\n  <component name=\"RunManager\" selected=\"Python.TODO_analyses\">\n    <configuration name=\"AN_analyses\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"intro2nlp_assignment1_code\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/AN_analyses.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"TODO_analyses\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"intro2nlp_assignment1_code\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/TODO_analyses.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"TODO_baselines\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"intro2nlp_assignment1_code\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/TODO_baselines.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"TODO_detailed_evaluation\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"intro2nlp_assignment1_code\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/TODO_detailed_evaluation.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"evaluate\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"intro2nlp_assignment1_code\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/evaluate.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <recent_temporary>\n      <list>\n        <item itemvalue=\"Python.TODO_analyses\" />\n        <item itemvalue=\"Python.AN_analyses\" />\n        <item itemvalue=\"Python.TODO_baselines\" />\n        <item itemvalue=\"Python.TODO_detailed_evaluation\" />\n        <item itemvalue=\"Python.evaluate\" />\n      </list>\n    </recent_temporary>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"SvnConfiguration\">\n    <configuration />\n  </component>\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"4e5420f3-a0e5-4b71-8c71-0dab952bcd35\" name=\"Default Changelist\" comment=\"\" />\n      <created>1646050952780</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1646050952780</updated>\n      <workItem from=\"1646050955347\" duration=\"1808000\" />\n      <workItem from=\"1646052847374\" duration=\"2389000\" />\n      <workItem from=\"1646386011328\" duration=\"4871000\" />\n      <workItem from=\"1647000280044\" duration=\"565000\" />\n      <workItem from=\"1648712195025\" duration=\"4281000\" />\n      <workItem from=\"1649078505569\" duration=\"1471000\" />\n    </task>\n    <task id=\"LOCAL-00001\" summary=\"Adjustment Word doc\">\n      <created>1650358379995</created>\n      <option name=\"number\" value=\"00001\" />\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1650358379995</updated>\n    </task>\n    <task id=\"LOCAL-00002\" summary=\"Adjustments Word\">\n      <created>1650360170214</created>\n      <option name=\"number\" value=\"00002\" />\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1650360170215</updated>\n    </task>\n    <task id=\"LOCAL-00003\" summary=\"Addition to 11\">\n      <created>1650373352040</created>\n      <option name=\"number\" value=\"00003\" />\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1650373352040</updated>\n    </task>\n    <task id=\"LOCAL-00004\" summary=\"Additions word doc\">\n      <created>1650389179750</created>\n      <option name=\"number\" value=\"00004\" />\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1650389179750</updated>\n    </task>\n    <task id=\"LOCAL-00005\" summary=\"Minor changes\">\n      <created>1650457973765</created>\n      <option name=\"number\" value=\"00005\" />\n      <option name=\"presentableId\" value=\"LOCAL-00005\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1650457973765</updated>\n    </task>\n    <option name=\"localTasksCounter\" value=\"6\" />\n    <servers />\n  </component>\n  <component name=\"TypeScriptGeneratedFilesManager\">\n    <option name=\"version\" value=\"2\" />\n  </component>\n  <component name=\"Vcs.Log.Tabs.Properties\">\n    <option name=\"TAB_STATES\">\n      <map>\n        <entry key=\"MAIN\">\n          <value>\n            <State />\n          </value>\n        </entry>\n      </map>\n    </option>\n  </component>\n  <component name=\"VcsManagerConfiguration\">\n    <MESSAGE value=\"Adjustment Word doc\" />\n    <MESSAGE value=\"Adjustments Word\" />\n    <MESSAGE value=\"Addition to 11\" />\n    <MESSAGE value=\"Additions word doc\" />\n    <MESSAGE value=\"Minor changes\" />\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"Minor changes\" />\n  </component>\n  <component name=\"XDebuggerManager\">\n    <breakpoint-manager>\n      <breakpoints>\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\n          <url>file://$PROJECT_DIR$/build_vocab.py</url>\n          <line>14</line>\n          <option name=\"timeStamp\" value=\"1\" />\n        </line-breakpoint>\n      </breakpoints>\n    </breakpoint-manager>\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 33a7bb4ce7072041bcfbb418dac7b3bb022f9ace)
+++ b/.idea/workspace.xml	(date 1650624378670)
@@ -1,7 +1,11 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
   <component name="ChangeListManager">
-    <list default="true" id="4e5420f3-a0e5-4b71-8c71-0dab952bcd35" name="Default Changelist" comment="Minor changes" />
+    <list default="true" id="4e5420f3-a0e5-4b71-8c71-0dab952bcd35" name="Default Changelist" comment="Minor changes">
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/AN_analyses.py" beforeDir="false" afterPath="$PROJECT_DIR$/AN_analyses.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/TODO_analyses.py" beforeDir="false" afterPath="$PROJECT_DIR$/TODO_analyses.py" afterDir="false" />
+    </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
     <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
@@ -44,7 +48,7 @@
       <recent name="$PROJECT_DIR$/data/preprocessed/val" />
     </key>
   </component>
-  <component name="RunManager" selected="Python.TODO_analyses">
+  <component name="RunManager" selected="Python.AN_analyses">
     <configuration name="AN_analyses" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="intro2nlp_assignment1_code" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -153,9 +157,9 @@
     </configuration>
     <recent_temporary>
       <list>
-        <item itemvalue="Python.TODO_analyses" />
         <item itemvalue="Python.AN_analyses" />
         <item itemvalue="Python.TODO_baselines" />
+        <item itemvalue="Python.TODO_analyses" />
         <item itemvalue="Python.TODO_detailed_evaluation" />
         <item itemvalue="Python.evaluate" />
       </list>
@@ -214,7 +218,14 @@
       <option name="project" value="LOCAL" />
       <updated>1650457973765</updated>
     </task>
-    <option name="localTasksCounter" value="6" />
+    <task id="LOCAL-00006" summary="Minor changes">
+      <created>1650458096339</created>
+      <option name="number" value="00006" />
+      <option name="presentableId" value="LOCAL-00006" />
+      <option name="project" value="LOCAL" />
+      <updated>1650458096339</updated>
+    </task>
+    <option name="localTasksCounter" value="7" />
     <servers />
   </component>
   <component name="TypeScriptGeneratedFilesManager">
